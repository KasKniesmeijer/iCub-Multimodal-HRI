# iCub-Multimodal-HRI
A repository for iCub Multimodal HRI.
#### Abstract
This study uses multisensory data (i.e., color and depth) to recognize human actions in the context of multimodal human-robot interaction. Here we employed the iCub robot to observe the predefined actions of the human partners by using four different tools on 20 objects. We show that the proposed multimodal ensemble learning leverages complementary characteristics of three color cameras and one depth sensor that improves, in most cases, recognition accuracy compared to the models trained with a single modality.

This repository is for the interested researches to reproduce the results.

#### Folder and file descriptions
* Code:               Preprocessing, Modelling and Ensemble learning code.
* Confusion Matrices: Multimodal and unimodal confusion matrices.   


#### Dataset 
Dataset Link: [https://www.crossvalidate.me/datasets/](https://www.crossvalidate.me/datasets/)




<!-- CONTACT -->
#### Contact


Kas Kniesmeijer - kaskniesmeijer@gmail.com

Project Link: [https://github.com/KasKniesmeijer/iCub-Multimodal-HRI](https://github.com/KasKniesmeijer/iCub-Multimodal-HRI)

<p align="right">(<a href="#readme-top">back to top</a>)</p>
